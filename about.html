<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>About emot.io</title>
    <link rel="stylesheet" href="style.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.min.css">
    <nav>
      <div class="nav-wrapper">
        <a href="index.html" class="brand-logo">emot.io</a>
        <ul id="nav-mobile" class="right hide-on-med-and-down">
          <li><a href="about.html">about</a></li>
        </ul>
      </div>
    </nav>  
</head>

  <body>
    <header>
      <div class="container">
          <h1>about emot.io</h1>
          <p class="sub-heading">emotions behind the words</p>
      </div>
    </header>
    <main >
        <div class="container">
            <div class="flow-text">
                emot.io is a platform that takes in a user-given input (video, audio, or image) and reports
                the emotions displayed by a person in the input file back to the user. This project is made 
                possible by Microsoft Cognitive Services, most notably the Emotion API, __, and ___. 
                <br>
                <br>
                This project was ideated at DubHacks 2017.                
                <br>
                <br>
                This project is meant to help those who have difficulty recognizing emotions from facial cues by 
                reporting on emotions from the user-given input. In a live setting, emot.io will reflect real-time
                emotions through an overlay feedback reporting the emotion(s) detected. 
            </div>
        </div>
    </main>
    
<footer class="container">
    <div class="footer">
        The Team: Zhanna Voloshina, Jessica Libman, Sarah Feldmann, and Joycie Yu from
        the University of Washington Information School 2017.
    </div>
</footer>
</html>